{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unlabeled Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create twitter hashtag reader and reddit thread reader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pick sample hashtags and threads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Twitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "import twint\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tweets(search, limit=1000):\n",
    "    \n",
    "    c = twint.Config()\n",
    "    c.Limit = limit\n",
    "    c.Min_likes = 5\n",
    "    c.Pandas = True\n",
    "    c.Lang = 'en'\n",
    "    c.Hide_output = True\n",
    "    c.Search = search\n",
    "    \n",
    "    twint.run.Search(c)\n",
    "    tweets = twint.storage.panda.Tweets_df\n",
    "    tweets = tweets.loc[tweets['language']=='en']\n",
    "    \n",
    "    return tweets['tweet'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model('final_model/model.h5')\n",
    "model.load_weights('final_model/model_weights.h5')\n",
    "vectorizer = load(open('final_model/vectorizer.pk', 'rb'))\n",
    "    \n",
    "def get_twitter_predictions(search, limit=1000):\n",
    "    tweets = get_tweets(search, limit)\n",
    "    X = process_data(tweets)\n",
    "    \n",
    "    X_processed = vectorizer.transform(process_data(X)).todense()\n",
    "    y_pred = model.predict_classes(X_processed)\n",
    "    df = pd.DataFrame(zip(X, y_pred.flatten()), columns=['tweet', 'positive'])\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Andrew\\anaconda3\\envs\\capstone\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
      "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
     ]
    }
   ],
   "source": [
    "df = get_twitter_predictions('#StateofPlay')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    0.958606\n",
       "0    0.041394\n",
       "Name: positive, dtype: float64"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['positive'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['trying to call upon my superpower from last year immediate announcement of final fantasy xvi to will an update into existence during today stateofplay',\n",
       " 'im expecting absolutely nothing from playstation today stateofplay httpstcoizg9xexysi',\n",
       " 'stateofplay is later today kid are yall ready for it i know i am game i reeeeally hope to see god of war ragnar k final fantasy xvi horizon forbidden west kena bridge of spirit holding out for an the order 1886 sequel too but i know it unlikely',\n",
       " 'the newest playstation state of play is nearly here join me live a we watch and react to all the game being shown stateofplay stream link httpstco2wuyonqhs8 httpstcogdeavntnzz',\n",
       " 'a little over 2 hour left make sure if you didnt check out my video to do so a little primer for stateofplay httpstcoz1swtvhlu8 httpstcosr7ca4ibqv']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['tweet'][df[df['positive']==True].index].tolist()[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['oh there a stateofplay today they really advertised that badly eh',\n",
       " '3 hour remain stateofplay',\n",
       " 'live from 945pm gmt join ianhigton zoedels and aoifelockhart a we find out what sony ha up it sleeve in the latest stateofplay stream expect announcement deepdives and more httpstcoo27wbfokxi httpstcomxp8vmilts',\n",
       " 'report playstation plus lineup reveal today either during stateofplay or a blog post immediately after the show',\n",
       " 'i dont think we see horizonforbiddenwest in this stateofplay hfw is too big of a game to try and cram into a 30 min show save it for e3 to demo it properly ps5']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['tweet'][df[df['positive']==False].index].tolist()[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reddit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from praw import Reddit\n",
    "from config import reddit_api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_comments(url):\n",
    "    reddit = Reddit(client_id=reddit_api['client_id'],\n",
    "                    client_secret=reddit_api['client_secret'],\n",
    "                    user_agent=reddit_api['user_agent'])\n",
    "\n",
    "    submissionId = url[url.find('comments'):].split('/')[1]\n",
    "    submission = reddit.submission(submissionId)\n",
    "    submission.comments.replace_more(limit=None)\n",
    "    comments = []\n",
    "    for comment in submission.comments:\n",
    "        comments.append(comment.body)\n",
    "        \n",
    "    return comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X = get_comments('https://www.reddit.com/r/Games/comments/ls33hs/activision_warns_a_standard_500gb_ps4_may_no/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load best processing and model pickles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Andrew\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Andrew\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Andrew\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "import pandas as pd\n",
    "from pickle import load\n",
    "import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(X):\n",
    "    X_pre = list(map(preprocessing.remove_markdown, X))\n",
    "    X_pre = list(map(preprocessing.remove_punctuation, X_pre))\n",
    "    X_pre = list(map(preprocessing.tokenize, X_pre))\n",
    "    X_pre = list(map(preprocessing.lemmatize, X_pre))\n",
    "    X_join = [' '.join(x) for x in X_pre]\n",
    "    \n",
    "    return X_join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model('final_model/model.h5')\n",
    "model.load_weights('final_model/model_weights.h5')\n",
    "vectorizer = load(open('final_model/vectorizer.pk', 'rb'))\n",
    "    \n",
    "def get_reddit_predictions(url):\n",
    "    comments = get_comments(url)\n",
    "    X = process_data(comments)\n",
    "    \n",
    "    X_processed = vectorizer.transform(process_data(X)).todense()\n",
    "    y_pred = model.predict_classes(X_processed)\n",
    "    df = pd.DataFrame(zip(X, y_pred.flatten()), columns=['comment', 'positive'])\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sample reddit thread ideas:\n",
    "- https://www.reddit.com/r/nintendo/comments/lm6obv/project_triangle_strategy_announcement_trailer/\n",
    "- https://www.reddit.com/r/nintendo/comments/lru33p/the_animal_crossing_new_horizons_free_update_is/\n",
    "- https://www.reddit.com/r/nintendo/comments/lqlrqg/tony_hawks_pro_skater_1_2_coming_to_nintendo/\n",
    "- https://www.reddit.com/r/Games/comments/ls6qlh/bravery_default_ii_review_thread/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = get_reddit_predictions('https://www.reddit.com/r/Games/comments/lm6ns2/project_triangle_strategy_announcement_trailer/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['positive'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['comment'][df[df['positive']==True].index].tolist()[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['comment'][df[df['positive']==False].index].tolist()[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "run data through processing and model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "output class percentages and numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "output unlabeled topic analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CREATE FRONTEND!!!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "capstone",
   "language": "python",
   "name": "capstone"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
