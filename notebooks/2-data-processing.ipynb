{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to reading in the data from feather files (smaller file sizes than csv), I also perform the train-test split here. Perhaps this could have been done after the processing, but I wanted to make sure the different processed files were in the same order."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of the functions I use are the same as in the sample dataset. I've functionalized them in scripts/preprocessing.py, so I can later use them with the target data. These are seperated out instead of put in a pipeline to help with debugging errors. The large data size caused many errors and long runtime, so running these steps individually was the best way to make it work. Not using pipelines now may also allow me to not use scikit-learn in a final product, which could help in getting all the libraries I need loaded onto heroku."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This allows importing of scripts, which are stored in a folder one level up\n",
    "import sys\n",
    "sys.path.append('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Andrew\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Andrew\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Andrew\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scripts import preprocessing\n",
    "from string import punctuation\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords_list = stopwords.words('english') + list(punctuation) + ['`', '’', '…', '\\n']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 of 10 started\n",
      "2 of 10 started\n",
      "3 of 10 started\n",
      "4 of 10 started\n",
      "5 of 10 started\n",
      "6 of 10 started\n",
      "7 of 10 started\n",
      "8 of 10 started\n",
      "9 of 10 started\n",
      "10 of 10 started\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print(i+1, 'of 10 started')\n",
    "    df = pd.read_pickle(f'../data/reviews_raw_{str(i)}.pkl.gz')\n",
    "    \n",
    "    y = df['voted_up'].to_numpy()\n",
    "    pd.DataFrame(y, columns=['voted_up']).to_pickle(f'../data/processed/y_{str(i)}.pkl.gz')\n",
    "    \n",
    "    X = df['review'].to_numpy()\n",
    "    X = list(map(preprocessing.remove_markdown, X))\n",
    "    X = list(map(preprocessing.remove_punctuation, X))\n",
    "    X = list(map(preprocessing.tokenize, X))\n",
    "    pd.DataFrame([' '.join(review) for review in X]).to_pickle(f'../data/processed/X_preprocessed_{str(i)}.pkl.gz')\n",
    "    \n",
    "    X_stopword = []\n",
    "    for review in X:\n",
    "        X_stopword.append([word for word in review if word not in stopwords_list])\n",
    "    pd.DataFrame([' '.join(review) for review in X_stopword]).to_pickle(f'../data/processed/X_stopword_{str(i)}.pkl.gz')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train-Test Split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To perform the train-test split, I combine all 10 of each data split, then run sklearn's train-test split function. I use a random state so that the data will split the same on all three data sets, as they are the same size. This is important because my computer cannot handle loading all three together, and so cannot run the train-test split on the entire dataset at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting y\n",
      "Starting X_stopword\n",
      "Starting X_preprocessed\n"
     ]
    }
   ],
   "source": [
    "for data_name in ['y', 'X_stopword', 'X_preprocessed']:\n",
    "    print('Starting', data_name)\n",
    "    df = pd.read_pickle(f'../data/processed/{data_name}_0.pkl.gz')\n",
    "    \n",
    "    for i in range(1, 10):\n",
    "        df = df.append(pd.read_pickle(f'../data/processed/{data_name}_{str(i)}.pkl.gz'))\n",
    "        \n",
    "    df_train, df_test = train_test_split(df, test_size=0.3, random_state=404)\n",
    "    df_train.to_pickle(f'../data/processed/{data_name}_train.pkl.gz')\n",
    "    df_test.to_pickle(f'../data/processed/{data_name}_test.pkl.gz')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I already know that TF-IDF performs the best, but I'm still interested to see how neural networks perform with the gensim document embeddings. These embeddings are much quicker and smaller than the TF-IDF vectorizers, so it isn't any trouble to run and save the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf = TfidfVectorizer(max_features=8000, stop_words=stopwords_list)\n",
    "X_train_tf = pd.DataFrame(tf.fit_transform(X_train_join).todense(), columns=tf.get_feature_names())\n",
    "X_test_tf = pd.DataFrame(tf.transform(X_test_join).todense(), columns=tf.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tf.to_feather('../data/processed/X_train_tf.feather')\n",
    "X_test_tf.to_feather('../data/processed/X_test_tf.feather')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF with Bigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF-IDF with Bigrams performed the best after running the models, so I pickled the vectorizer to use again later. When I get the ability to run bigger models and vectorizers, I may come back and try other levels of n-grams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pickle import dump, load\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_bigram = TfidfVectorizer(max_features=1000, ngram_range=(1,2))\n",
    "X_train = pd.read_pickle('../data/processed/X_preprocessed_train.pkl.gz')[0]\n",
    "X_train_bigram = pd.DataFrame(tf_bigram.fit_transform(X_train).todense(), columns=tf_bigram.get_feature_names())\n",
    "\n",
    "X_train_bigram.to_pickle('../data/processed/X_bigram_train.pkl.gz')\n",
    "dump(tf_bigram, open('../final_model/tfidf_bigram_vectorizer.pk', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_bigram = load(open('../final_model/tfidf_bigram_vectorizer.pk', 'rb'))\n",
    "X_test = pd.read_pickle('../data/processed/X_preprocessed_test.pkl.gz')[0]\n",
    "X_test_bigram = pd.DataFrame(tf_bigram.transform(X_test).todense(), columns=tf_bigram.get_feature_names())\n",
    "X_test_bigram.to_pickle('../data/processed/X_bigram_test.pkl.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_bigram.to_pickle('../data/processed/X_bigram_train.pkl.gz')\n",
    "X_test_bigram.to_pickle('../data/processed/X_bigram_test.pkl.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dump(tf_bigram, open('../final_model/tfidf_bigram_vectorizer.pk', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Document Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.sklearn_api import D2VTransformer\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d2v = D2VTransformer()\n",
    "X_train_embed = d2v.fit_transform(X_train_pre)\n",
    "X_test_embed = d2v.transform(X_test_pre)\n",
    "\n",
    "scaler = MinMaxScaler((1, 2))\n",
    "X_train_embed = pd.DataFrame(scaler.fit_transform(X_train_embed))\n",
    "X_test_embed = pd.DataFrame(scaler.transform(X_test_embed))\n",
    "\n",
    "X_train_embed.columns = X_train_embed.columns.astype(str)\n",
    "X_test_embed.columns = X_test_embed.columns.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_embed.to_feather('../data/processed/X_train_embed.feather')\n",
    "X_test_embed.to_feather('../data/processed/X_test_embed.feather')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of these final processed files are too alrge to upload to Github, so the entire data/processed folder has been added to .gitignore. You will need to run this script yourself to generate the same files. The raw data is still included in the Github upload."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "capstone",
   "language": "python",
   "name": "capstone"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
